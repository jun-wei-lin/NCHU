# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BgixIQ-GgTGylsBe-4aDK_E-mhLZ3z-y
"""

import pandas as pd
import torch
from torch.utils.data import Dataset
import torch.nn as nn

class DiscountDataset(Dataset):
    def __init__(self, csv_path):
        df = pd.read_csv(csv_path)
        self.state = df[["age", "region", "history"]].values / [60.0, 3.0, 1.0]  # 正規化
        self.action = df["action"].values
        self.reward = df["reward"].values
        self.next_state = self.state[1:].tolist() + [self.state[-1]]  # 下一狀態模擬 (簡化)
        self.done = [False] * (len(self.state) - 1) + [True]

    def __len__(self):
        return len(self.state)

    def __getitem__(self, idx):
        return (
            torch.tensor(self.state[idx], dtype=torch.float32),
            torch.tensor(self.action[idx], dtype=torch.long),
            torch.tensor(self.reward[idx], dtype=torch.float32),
            torch.tensor(self.next_state[idx], dtype=torch.float32),
            torch.tensor(self.done[idx], dtype=torch.float32)
        )
class QNetwork(nn.Module):
    def __init__(self, state_dim=3, action_dim=4):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim)
        )

    def forward(self, x):
        return self.model(x)

import torch
import torch.optim as optim
from torch.utils.data import DataLoader

def train_dqn_from_csv(csv_path, episodes=10, batch_size=64, gamma=0.99):
    dataset = DiscountDataset(csv_path)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    q_net = QNetwork()
    target_net = QNetwork()
    target_net.load_state_dict(q_net.state_dict())
    target_net.eval()

    optimizer = optim.Adam(q_net.parameters(), lr=1e-3)
    loss_fn = nn.MSELoss()

    for episode in range(episodes):
        for state, action, reward, next_state, done in dataloader:
            q_values = q_net(state)
            q_action = q_values.gather(1, action.unsqueeze(1)).squeeze(1)

            with torch.no_grad():
                max_next_q = target_net(next_state).max(1)[0]
                target_q = reward + gamma * max_next_q * (1 - done)

            loss = loss_fn(q_action, target_q)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        target_net.load_state_dict(q_net.state_dict())
        print(f"Episode {episode+1} loss: {loss.item():.4f}")

    return q_net

from sklearn.metrics import accuracy_score, classification_report
import numpy as np
def evaluate_accuracy_from_csv(model, csv_path):
    df = pd.read_csv(csv_path)

    # 正規化 state 輸入
    state = df[["age", "region", "history"]].values / [60.0, 3.0, 1.0]
    converted = df["converted"].values

    # 預測每個 state 的最佳行動（Q值最大）
    with torch.no_grad():
        state_tensor = torch.tensor(state, dtype=torch.float32)
        q_values = model(state_tensor)
        predicted_action = torch.argmax(q_values, dim=1).numpy()

    # 模擬預測：假設如果模型建議的折扣在資料中實際發生且有購買則為「正確」
    # → 這裡為簡化，我們直接以 `converted` 為 ground truth 與模型推薦動作無關
    # 更嚴謹做法是使用模型推薦的折扣產生資料再驗證轉換率

    # 模型預測轉換條件：Q(action).max 對應到的動作 → 該動作是否導致購買
    predicted_converted = []
    for i, a in enumerate(predicted_action):
        real_discount = df["discount"].iloc[i]
        predicted_discount = [0.0, 0.05, 0.1, 0.2][a]
        # 如果模型推薦的折扣與實際折扣一致 → 採用實際結果
        if np.isclose(predicted_discount, real_discount):
            predicted_converted.append(df["converted"].iloc[i])
        else:
            predicted_converted.append(0)  # 模型推薦但實際未給，視為未購買

    acc = accuracy_score(converted, predicted_converted)
    print(f"✅ 預測準確率（accuracy）: {acc:.4f}")
    print("\n📋 詳細分類報告：")
    print(classification_report(converted, predicted_converted, digits=4))

trained_q_net = train_dqn_from_csv("real_discount_data.csv", episodes=200, batch_size=128)
evaluate_accuracy_from_csv(trained_q_net, "real_discount_data.csv")