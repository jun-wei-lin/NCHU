{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNwnzqpRulqNhslzhiweWvF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jun-wei-lin/NCHU/blob/main/Deep%20Reinforcement%20Learning/HW4_DQN%20Variants/HW4_3%20Enhance%20DQN%20for%20random%20mode%20WITH%20Training%20Tips/dqn_keras_random.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GI87J7gCnxZ",
        "outputId": "e9cc984b-3267-4529-9c94-252b2e444106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/100 - Total Reward: 4.0\n",
            "Episode 2/100 - Total Reward: 2.0\n",
            "Episode 3/100 - Total Reward: 4.0\n",
            "Episode 4/100 - Total Reward: 1.0\n",
            "Episode 5/100 - Total Reward: 1.0\n",
            "Episode 6/100 - Total Reward: 4.0\n",
            "Episode 7/100 - Total Reward: 3.0\n",
            "Episode 8/100 - Total Reward: 2.0\n",
            "Episode 9/100 - Total Reward: 11.0\n",
            "Episode 10/100 - Total Reward: 5.0\n",
            "Episode 11/100 - Total Reward: 0.0\n",
            "Episode 12/100 - Total Reward: 10.0\n",
            "Episode 13/100 - Total Reward: 3.0\n",
            "Episode 14/100 - Total Reward: 1.0\n",
            "Episode 15/100 - Total Reward: 21.0\n",
            "Episode 16/100 - Total Reward: 1.0\n",
            "Episode 17/100 - Total Reward: 11.0\n",
            "Episode 18/100 - Total Reward: 3.0\n",
            "Episode 19/100 - Total Reward: 15.0\n",
            "Episode 20/100 - Total Reward: 19.0\n",
            "Episode 21/100 - Total Reward: 2.0\n",
            "Episode 22/100 - Total Reward: 16.0\n",
            "Episode 23/100 - Total Reward: 16.0\n",
            "Episode 24/100 - Total Reward: 1.0\n",
            "Episode 25/100 - Total Reward: 8.0\n",
            "Episode 26/100 - Total Reward: 11.0\n",
            "Episode 27/100 - Total Reward: 2.0\n",
            "Episode 28/100 - Total Reward: 8.0\n",
            "Episode 29/100 - Total Reward: 1.0\n",
            "Episode 30/100 - Total Reward: 22.0\n",
            "Episode 31/100 - Total Reward: 12.0\n",
            "Episode 32/100 - Total Reward: 18.0\n",
            "Episode 33/100 - Total Reward: 9.0\n",
            "Episode 34/100 - Total Reward: 18.0\n",
            "Episode 35/100 - Total Reward: 8.0\n",
            "Episode 36/100 - Total Reward: 7.0\n",
            "Episode 37/100 - Total Reward: 4.0\n",
            "Episode 38/100 - Total Reward: 7.0\n",
            "Episode 39/100 - Total Reward: 7.0\n",
            "Episode 40/100 - Total Reward: 1.0\n",
            "Episode 41/100 - Total Reward: 5.0\n",
            "Episode 42/100 - Total Reward: 1.0\n",
            "Episode 43/100 - Total Reward: 16.0\n",
            "Episode 44/100 - Total Reward: 7.0\n",
            "Episode 45/100 - Total Reward: 15.0\n",
            "Episode 46/100 - Total Reward: 1.0\n",
            "Episode 47/100 - Total Reward: 15.0\n",
            "Episode 48/100 - Total Reward: 1.0\n",
            "Episode 49/100 - Total Reward: 1.0\n",
            "Episode 50/100 - Total Reward: 2.0\n",
            "Episode 51/100 - Total Reward: 2.0\n",
            "Episode 52/100 - Total Reward: 14.0\n",
            "Episode 53/100 - Total Reward: 14.0\n",
            "Episode 54/100 - Total Reward: 8.0\n",
            "Episode 55/100 - Total Reward: 14.0\n",
            "Episode 56/100 - Total Reward: 1.0\n",
            "Episode 57/100 - Total Reward: 8.0\n",
            "Episode 58/100 - Total Reward: 1.0\n",
            "Episode 59/100 - Total Reward: 5.0\n",
            "Episode 60/100 - Total Reward: 2.0\n",
            "Episode 61/100 - Total Reward: 11.0\n",
            "Episode 62/100 - Total Reward: 7.0\n",
            "Episode 63/100 - Total Reward: 1.0\n",
            "Episode 64/100 - Total Reward: 6.0\n",
            "Episode 65/100 - Total Reward: 0.0\n",
            "Episode 66/100 - Total Reward: 2.0\n",
            "Episode 67/100 - Total Reward: 16.0\n",
            "Episode 68/100 - Total Reward: 0.0\n",
            "Episode 69/100 - Total Reward: 7.0\n",
            "Episode 70/100 - Total Reward: 1.0\n",
            "Episode 71/100 - Total Reward: 12.0\n",
            "Episode 72/100 - Total Reward: 16.0\n",
            "Episode 73/100 - Total Reward: 3.0\n",
            "Episode 74/100 - Total Reward: 2.0\n",
            "Episode 75/100 - Total Reward: 4.0\n",
            "Episode 76/100 - Total Reward: 4.0\n",
            "Episode 77/100 - Total Reward: 3.0\n",
            "Episode 78/100 - Total Reward: 6.0\n",
            "Episode 79/100 - Total Reward: 12.0\n",
            "Episode 80/100 - Total Reward: 13.0\n",
            "Episode 81/100 - Total Reward: 28.0\n",
            "Episode 82/100 - Total Reward: 12.0\n",
            "Episode 83/100 - Total Reward: 3.0\n",
            "Episode 84/100 - Total Reward: 8.0\n",
            "Episode 85/100 - Total Reward: 2.0\n",
            "Episode 86/100 - Total Reward: 7.0\n",
            "Episode 87/100 - Total Reward: 12.0\n",
            "Episode 88/100 - Total Reward: 6.0\n",
            "Episode 89/100 - Total Reward: 8.0\n",
            "Episode 90/100 - Total Reward: 6.0\n",
            "Episode 91/100 - Total Reward: 3.0\n",
            "Episode 92/100 - Total Reward: 5.0\n",
            "Episode 93/100 - Total Reward: 15.0\n",
            "Episode 94/100 - Total Reward: 6.0\n",
            "Episode 95/100 - Total Reward: 4.0\n",
            "Episode 96/100 - Total Reward: 6.0\n",
            "Episode 97/100 - Total Reward: 7.0\n",
            "Episode 98/100 - Total Reward: 4.0\n",
            "Episode 99/100 - Total Reward: 1.0\n",
            "Episode 100/100 - Total Reward: 1.0\n",
            "Test Episode 1: Total Reward = 1.0\n",
            "Test Episode 2: Total Reward = 2.0\n",
            "Test Episode 3: Total Reward = 12.0\n",
            "Test Episode 4: Total Reward = 4.0\n",
            "Test Episode 5: Total Reward = 9.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from collections import deque\n",
        "\n",
        "# üß™ Ê®°Êì¨Áî® Random Mode Áí∞Â¢ÉÔºàËµ∑ÈªûËàáÁí∞Â¢ÉÈö®Ê©üÔºâ\n",
        "class RandomGridEnv:\n",
        "    def __init__(self):\n",
        "        self.state_size = 4\n",
        "        self.action_size = 2\n",
        "\n",
        "    def reset(self):\n",
        "        return np.random.rand(self.state_size)\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state = np.random.rand(self.state_size)\n",
        "        reward = 1.0 if random.random() > 0.3 else 0.0\n",
        "        done = random.random() > 0.9\n",
        "        return next_state, reward, done, {}\n",
        "\n",
        "# Q-Network Âª∫Á´ã\n",
        "def create_q_model(state_size, action_size):\n",
        "    inputs = keras.Input(shape=(state_size,))\n",
        "    x = layers.Dense(64, activation='relu')(inputs)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    outputs = layers.Dense(action_size, activation='linear')(x)\n",
        "    return keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Replay Buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# Ë®ìÁ∑¥ÊµÅÁ®ã\n",
        "def train_dqn(env, episodes=100, gamma=0.99, epsilon=0.1, batch_size=32, buffer_capacity=1000):\n",
        "    state_size = env.state_size\n",
        "    action_size = env.action_size\n",
        "\n",
        "    q_model = create_q_model(state_size, action_size)\n",
        "    target_model = create_q_model(state_size, action_size)\n",
        "    target_model.set_weights(q_model.get_weights())\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=1e-3, clipnorm=1.0)  # Gradient clipping\n",
        "    loss_fn = keras.losses.MeanSquaredError()\n",
        "\n",
        "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=1e-3,\n",
        "        decay_steps=100,\n",
        "        decay_rate=0.96,\n",
        "        staircase=True\n",
        "    )\n",
        "\n",
        "    replay_buffer = ReplayBuffer(buffer_capacity)\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            if random.random() < epsilon:\n",
        "                action = random.randint(0, action_size - 1)\n",
        "            else:\n",
        "                q_values = q_model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
        "                action = np.argmax(q_values[0])\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            replay_buffer.add((state, action, reward, next_state, done))\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if len(replay_buffer) >= batch_size:\n",
        "                batch = replay_buffer.sample(batch_size)\n",
        "                states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "                states = np.array(states)\n",
        "                next_states = np.array(next_states)\n",
        "                rewards = np.array(rewards, dtype=np.float32)\n",
        "                dones = np.array(dones, dtype=np.float32)\n",
        "\n",
        "                next_qs = target_model.predict(next_states, verbose=0)\n",
        "                max_next_qs = np.max(next_qs, axis=1)\n",
        "                targets = rewards + (1 - dones) * gamma * max_next_qs\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    qs = q_model(states)\n",
        "                    selected_qs = tf.reduce_sum(qs * tf.one_hot(actions, action_size), axis=1)\n",
        "                    loss = loss_fn(targets, selected_qs)\n",
        "\n",
        "                grads = tape.gradient(loss, q_model.trainable_variables)\n",
        "                optimizer.learning_rate = lr_schedule(episode)\n",
        "                optimizer.apply_gradients(zip(grads, q_model.trainable_variables))\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            target_model.set_weights(q_model.get_weights())\n",
        "\n",
        "        print(f\"Episode {episode + 1}/{episodes} - Total Reward: {total_reward}\")\n",
        "\n",
        "    return q_model\n",
        "\n",
        "# Ê∏¨Ë©¶ÊµÅÁ®ã\n",
        "def test_policy(env, model, episodes=5):\n",
        "    for ep in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            q_values = model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
        "            action = np.argmax(q_values[0])\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "        print(f\"Test Episode {ep+1}: Total Reward = {total_reward}\")\n",
        "\n",
        "# ‰∏ªÁ®ãÂºèÂÖ•Âè£\n",
        "if __name__ == \"__main__\":\n",
        "    env = RandomGridEnv()\n",
        "    model = train_dqn(env)\n",
        "    test_policy(env, model)\n"
      ]
    }
  ]
}