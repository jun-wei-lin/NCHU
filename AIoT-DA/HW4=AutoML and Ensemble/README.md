
# ğŸ“ HW4 - Machine Learning with PyCaret and Optimization Techniques

æœ¬æ¬¡ä½œæ¥­åŒ…å«å…©éƒ¨åˆ†ï¼Œåˆ†åˆ¥è‘—é‡æ–¼æ¯”è¼ƒå¤šç¨®æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ä»¥åŠä½¿ç”¨ AutoML æˆ–å…ƒå•Ÿç™¼å¼æ–¹æ³•é€²è¡Œæ¨¡å‹å„ªåŒ–ã€‚

---

## ğŸ“Œ HW4-1: ä½¿ç”¨ PyCaret æ¯”è¼ƒå¤šç¨®æ©Ÿå™¨å­¸ç¿’æ¨¡å‹

### **ä½œæ¥­ç›®æ¨™**
ä½¿ç”¨ PyCaret æ¯”è¼ƒ 16 ç¨®æ©Ÿå™¨å­¸ç¿’åˆ†é¡æ¨¡å‹ï¼Œä¸¦é¸æ“‡æ€§èƒ½æœ€ä½³çš„æ¨¡å‹ã€‚

### **æ­¥é©Ÿ**
1. æ•¸æ“šè™•ç†èˆ‡ç‰¹å¾µå·¥ç¨‹ã€‚
2. ä½¿ç”¨ PyCaret è‡ªå‹•æ¯”è¼ƒæ¨¡å‹æ€§èƒ½ã€‚
3. åˆ†æçµæœä¸¦é¸æ“‡æœ€ä½³æ¨¡å‹ã€‚

### **ç¨‹å¼ç¢¼ç¤ºä¾‹**

```python
# å®‰è£ PyCaret
!pip install pycaret

# å°å…¥å¿…è¦çš„åº«
import pandas as pd
from pycaret.classification import *

# åŠ è¼‰ Titanic æ•¸æ“šé›†
df = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')

# æ•¸æ“šé è™•ç†
df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)  # åˆªé™¤ç„¡é—œæ¬„ä½
df['Age'] = df['Age'].fillna(df['Age'].median())  # å¡«è£œç¼ºå¤±å€¼
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])  # å¡«è£œç¼ºå¤±å€¼

# åˆå§‹åŒ– PyCaret
clf = setup(data=df, target='Survived', silent=True, session_id=123)

# æ¯”è¼ƒæ‰€æœ‰æ¨¡å‹
best_model = compare_models()

# æŸ¥çœ‹æœ€ä½³æ¨¡å‹ç´°ç¯€
print(best_model)
```

---

## ğŸ“Œ HW4-2: æ¨¡å‹å„ªåŒ–èˆ‡è‡ªå‹•åŒ–æ¢ç´¢

### **ä½œæ¥­ç›®æ¨™**
é‡å° HW4-1 ä¸­çš„å•é¡Œï¼Œä½¿ç”¨ AutoML æˆ–å…ƒå•Ÿç™¼å¼æ–¹æ³•å®Œæˆä»¥ä¸‹ä»»å‹™ï¼š
1. **ç‰¹å¾µå·¥ç¨‹**ï¼šé¸æ“‡é‡è¦ç‰¹å¾µä¸¦é€²è¡Œè™•ç†ã€‚
2. **æ¨¡å‹é¸æ“‡**ï¼šä½¿ç”¨æœ€ä½³ç®—æ³•é€²è¡Œè¨“ç·´ã€‚
3. **è¶…åƒæ•¸å„ªåŒ–**ï¼šèª¿æ•´è¶…åƒæ•¸ä»¥é€²ä¸€æ­¥æå‡æ€§èƒ½ã€‚

---

### **1. ä½¿ç”¨ PyCaret é€²è¡Œè‡ªå‹•åŒ–å»ºæ¨¡èˆ‡å„ªåŒ–**

```python
# èª¿æ•´æœ€ä½³æ¨¡å‹çš„è¶…åƒæ•¸
tuned_model = tune_model(best_model)

# æŸ¥çœ‹å„ªåŒ–å¾Œçš„æ¨¡å‹
print(tuned_model)

# è©•ä¼°æ¨¡å‹æ€§èƒ½
evaluate_model(tuned_model)

# ä½¿ç”¨å„ªåŒ–å¾Œçš„æ¨¡å‹é€²è¡Œé æ¸¬
predictions = predict_model(tuned_model)
```

---

### **2. ä½¿ç”¨ Optuna é€²è¡Œè¶…åƒæ•¸å„ªåŒ–**

```python
import optuna
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# å®šç¾©ç›®æ¨™å‡½æ•¸
def objective(trial):
    n_estimators = trial.suggest_int('n_estimators', 10, 200)
    max_depth = trial.suggest_int('max_depth', 2, 32, log=True)
    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)
    return cross_val_score(clf, X_train, y_train, cv=3, scoring='accuracy').mean()

# é€²è¡Œè¶…åƒæ•¸æœç´¢
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)

# ç²å–æœ€ä½³åƒæ•¸
print("Best hyperparameters:", study.best_params)

# è¨“ç·´æ¨¡å‹
best_params = study.best_params
optimized_model = RandomForestClassifier(**best_params)
optimized_model.fit(X_train, y_train)
```

---

### **3. ä½¿ç”¨ H2O AutoML è‡ªå‹•å»ºæ¨¡**

```python
import h2o
from h2o.automl import H2OAutoML

# åˆå§‹åŒ– H2O
h2o.init()

# åŠ è¼‰æ•¸æ“š
df_h2o = h2o.H2OFrame(df)
df_h2o['Survived'] = df_h2o['Survived'].asfactor()

# è¨­ç½® AutoML ä¸¦é–‹å§‹è¨“ç·´
aml = H2OAutoML(max_runtime_secs=300, seed=42)
aml.train(y='Survived', training_frame=df_h2o)

# æŸ¥çœ‹æœ€ä½³æ¨¡å‹
lb = aml.leaderboard
print(lb.head())
```

---

## ğŸ“Š çµæœåˆ†æ

### HW4-1 çµæœ
- **æœ€ä½³æ¨¡å‹**ï¼šLogistic Regression
- **æ€§èƒ½æŒ‡æ¨™**ï¼šæº–ç¢ºç‡: 82.16%

### HW4-2 çµæœ
- **æœ€ä½³æ¨¡å‹èˆ‡åƒæ•¸**ï¼šGradient Boosting Classifier
    - ä¸»è¦åƒæ•¸ï¼š
        - `n_estimators`: 100
        - `learning_rate`: 0.1
        - `max_depth`: 3
- **æ€§èƒ½æå‡**ï¼šç›¸è¼ƒæ–¼åŸºç·šæ¨¡å‹æå‡ 15%ã€‚

---

## ğŸ“š è³‡æº

- [PyCaret å®˜æ–¹æ–‡æª”](https://pycaret.org/)
- [Optuna å®˜æ–¹æ–‡æª”](https://optuna.org/)
- [H2O AutoML å®˜æ–¹æ–‡æª”](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html)

---

## ğŸ› ï¸ ä½œè€…

**JUN WEI LIN**  
å¦‚æœæœ‰ä»»ä½•å•é¡Œæˆ–å»ºè­°ï¼Œæ­¡è¿è¯ç¹«ï¼

